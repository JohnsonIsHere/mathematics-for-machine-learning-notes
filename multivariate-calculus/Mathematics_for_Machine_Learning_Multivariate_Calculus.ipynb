{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8dc64a",
   "metadata": {},
   "source": [
    "# A. Introduction to Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd10a63",
   "metadata": {},
   "source": [
    "## A1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acffb34",
   "metadata": {},
   "source": [
    "*  What are functions? \n",
    "   *  A function is a mathematical relationship between inputs and an output. It can be thought of as a machine that takes in one or more variables and produces a single, corresponding result. For example, a function for the temperature of a room might take in the coordinates ($x$,$y$,$z$) and time ($t$) as inputs and return the temperature at that specific point and time.\n",
    "   *  The notation $f(x)$ represents \"f as a function of x\", not \"f multiplied by x.\" This can be a point of confusion due to its seemingly arbitrary nature, but it's a standard convention in mathematical language.\n",
    "* The creative essence of science\n",
    "  * Selecting a function to model real-world data is a core, creative step in science and machine learning. This process involves formulating a **hypothesis**—a candidate function that could represent the relationship you're observing. Without this initial creative step, there would be nothing to test or investigate.\n",
    "* Introduction to Calculus\n",
    "  * **Calculus** is the study of how functions change with respect to their input variables. It provides a set of tools to investigate and manipulate these functions. By understanding calculus, you can analyze the behavior of functions and use them to model complex phenomena in the real world.\n",
    "* Gradients and Derivatives\n",
    "  * A great way to visualize this concept is with a **speed-time graph**.\n",
    "  * ![Speed time graph](images/speed_time_graph.png)\n",
    "\n",
    "    * The **gradient** (or slope) of the graph at any point represents the **rate of change** of speed with respect to time, which is the **acceleration**.\n",
    "    * A positive gradient indicates acceleration, a negative gradient indicates deceleration, and a zero gradient (a flat horizontal line) means constant speed with zero acceleration.\n",
    "    * The gradient at a single point is called the **local gradient** and can be visualized as the slope of a tangent line that touches the curve at that point.\n",
    "  * By finding the local gradient at every point on a continuous function, we can create an entirely new function called its **derivative**. The derivative describes the original function's slope at every point.\n",
    "* Higher-Order Derivatives and Anti-Derivatives\n",
    "  * This process can be repeated. The derivative of the acceleration function is called the **jerk**, which represents the rate of change of acceleration. This concept is useful for describing the \"jerky\" motion of a car as it starts and stops. The jerk is the second derivative of the speed-time function.\n",
    "  * The inverse procedure, finding a function for which our original function is the derivative, is called the **anti-derivative**. For our speed-time example, the anti-derivative would be the **distance-time functio**n, as the rate of change of distance is speed. The anti-derivative is closely related to an integral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c03f1",
   "metadata": {},
   "source": [
    "## A2. Derivatives (Sum Rule and Power Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d01ce8",
   "metadata": {},
   "source": [
    "* Defining the Derivative\n",
    "  * The derivative is the formal mathematical notation for the gradient of a function. For a linear function with a constant gradient, the slope is defined as \"**rise over run**.\"\n",
    "  * For a non-linear function where the gradient changes at every point, we define the derivative at a specific point $x$ by taking the limit of the \"rise over run\" formula. We consider a second point that is an infinitesimally small distance $Δx$ away from the first point. As this distance approaches zero, the line connecting the two points becomes a perfect approximation of the tangent line at point $x$. $$ \\frac{df}{dx} = f'(x) = \\lim_{Δx\\to0} (\\frac{f(x+Δx)-f(x)}{Δx}) $$\n",
    "    * The notation for the derivative can be either $f'(x)$ (read as \"f prime of x\")\n",
    "    * or $\\frac{df}{dx}$ (read as \"df by dx\"). \n",
    "    * The key idea is that we are not dividing by zero, but rather observing the behavior of the expression as $Δx$ gets extremely close to zero.\n",
    "* Fundamental Rules of Differentiation\n",
    "  * This definition, while powerful, can be tedious to apply directly to every function. Fortunately, we can derive and use general rules to simplify the process.\n",
    "  * **The Sum Rule**\n",
    "    * The derivative of a sum of functions is the sum of their individual derivatives. This means you can differentiate each term in a function separately and then add the results together.\n",
    "    * Example: The derivative of $f(x)=3x+2$ is the derivative of $3x$ plus the derivative of $2$.\n",
    "  * **The Power Rule**\n",
    "    * For a function in the form of $f(x) = ax^b$, its derivative is:\n",
    "    $$ f(x) = ax^b $$\n",
    "    $$ f'(x) = abx^{(b-1)} $$\n",
    "    * The rule is: multiply the coefficient by the original power, and then subtract 1 from the power.\n",
    "    * Example: For $f(x) = 5x^2$, the derivative is $f'(x)=(5)(2)x^{2-1}=10x^1=10x$\n",
    "* Special cases\n",
    "  * The Derivative of $\\frac{1}{x}$\n",
    "    * ![Derivative - Discontiunity ](images/derivative_discontinuity.png)\n",
    "    * The Derivative of $\\frac{1}{x}$ has a **discontinuity** at $x=0$, as division by zero is undefined. However, we can find its derivative using the limit definition of differentiation. After working through the algebra, the derivative is found to be: \n",
    "    $$ f(x) = \\frac{1}{x} $$\n",
    "    $$ f'(x) = \\lim_{Δx\\to0} (\\frac{ \\frac{1}{x+Δx} - \\frac{1}{x} }{Δx}) $$\n",
    "    $$ = \\lim_{Δx\\to0} (\\frac{ \\frac{x}{x(x+Δx)} - \\frac{x+Δx}{x(x+Δx)} }{Δx}) $$\n",
    "    $$ = \\lim_{Δx\\to0}  (\\frac{\\frac{-Δx}{x(x+Δx)}}{Δx}) $$\n",
    "    $$ = \\lim_{Δx\\to0} (\\frac{-1}{x^2+xΔx}) $$\n",
    "    $$ = - \\frac{1}{x^2}$$\n",
    "    * This derivative function is always negative, matching our visual observation that the original function's slope is always decreasing. Like the original function, the derivative is also undefined at $x = 0$.\n",
    "  * The Exponential Function ($e^x$)\n",
    "    * ![Derivative - Euler's Nimber ](images/derivative_eulers_number.png)\n",
    "    * The exponential function, $f(x) = e^x$, has  a unique and powerful property: **its derivative is itself**. $$ \\frac{d}{dx} e^x = e^x$$ \n",
    "    * This means the value of the function at any point is equal to its slope at that same point. This self-similarity is incredibly useful in calculus and other areas of mathematics. The constant $e$ (Euler's number), approximately 2.718, is fundamental to this function.\n",
    "      * The \"Designed\" Nature of the Exponential Function $e^x$\n",
    "        * The constant $e$ (Euler's number) is not a random value; it's specifically defined to satisfy a unique and powerful property in calculus. Its entire purpose is to make the derivative of the exponential function $f(x)=e^x$ equal to itself.\n",
    "      * Derivation of the Derivative\n",
    "        * We can see this by using the formal limit definition of the derivative for a general exponential function $f(x)=a^x$.$$ f'(x)=\\lim_{Δx\\to0} \\frac{f(x+Δx)-f(x)}{Δx} = \\lim_{Δx\\to0} \\frac{a^{x+Δx}-a^x}{Δx}$$\n",
    "        * Using the rule of exponents ($a^{x+y} =a^xa^y$), we can factor out $a^x$: $$ f'(x)=\\lim_{Δx\\to0} \\frac{a^x \\cdot a^{Δx}-a^x}{Δx} = a^x \\lim_{Δx\\to0} \\frac{a^{Δx}-1}{Δx} $$.\n",
    "        * For most values of the base $a$, the limit part of this expression will evaluate to some constant value other than 1.\n",
    "        * The number $e$ is precisely the value for the base a that makes this limit exactly 1: $$ \\lim_{Δx\\to0} \\frac{e^{Δx}-1}{Δx} $$\n",
    "        * Therefore, when we substitute $a=e$ back into our derivative expression, we get: $$ f'(x) = e^x \\cdot 1 = e^x $$\n",
    "      * This result shows that the rate of change of the function $e^x$ at any point is simply the value of the function itself at that point. This isn't a coincidence; it's the very reason why $e$ is so fundamental to calculus and the modeling of natural growth and decay processes.\n",
    "  * Trigonometric Functions (Sine and Cosine)\n",
    "    * ![Derivative - Sin and Cos ](images/derivative_sin_cos.png)\n",
    "    * The trigonometric functions sine and cosine have an interesting relationship when differentiated. They follow a cyclical pattern:\n",
    "      * The derivative of $\\sin(x)$ is $\\cos(x)$.\n",
    "      * The derivative of $\\cos(x)$ is $-\\sin(x)$.\n",
    "      * The derivative of $-\\sin(x)$ is $-\\cos(x)$.\n",
    "      * The derivative of $-\\cos(x)$ is $\\sin(x)$.\n",
    "    * After four differentiations, the function returns to its original form. This self-similarity is a hint that these functions are deeply related to the exponential function, although the connection is not immediately obvious.\n",
    "  * Ultimately, these examples demonstrate that even with complex functions, the core concept of differentiation remains the same: finding the \"rise over run\" at every point on the curve.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef9bb8",
   "metadata": {},
   "source": [
    "## A3. The Product Rule of Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4805534",
   "metadata": {},
   "source": [
    "* The product rule is a shortcut for finding the derivative of a function that is the product of two separate functions, $A(x)=f(x)g(x)$. Instead of using the tedious limit definition, we can visualize the rule by thinking about the change in area of a rectangle with sides $f(x)$ and $g(x)$.\n",
    "* ![Product Rule](images/product_rule.png)\n",
    "* When we increase $x$ by a small amount $Δx$, the area of the rectangle changes. The increase in area, $ΔA$, consists of three parts:\n",
    "  * A vertical strip with area $f(x)(g(x+Δx)-g(x))$.\n",
    "  * A horizontal strip with area $g(x)(f(x+Δx)-f(x))$\n",
    "  * A small corner rectangle with area $(f(x+Δx)-f(x))(g(x+Δx)-g(x))$ \n",
    "* So, the whole $ΔA$ will be: $$ΔA = f(x)(g(x+Δx)-g(x)) +$$ $$g(x)(f(x+Δx)-f(x)) +$$ $$(f(x+Δx)-f(x))(g(x+Δx)-g(x))$$\n",
    "* As $Δx$ approaches zero, the area of the smallest corner rectangle $(f(x+Δx)-f(x))(g(x+Δx)-g(x))$ becomes negligible compared to the other two parts and can be ignored in the limit. \n",
    "  $$ \\lim_{Δx\\to0} (ΔA(x)) = \\lim_{Δx\\to0} ( f(x)(g(x+Δx)-g(x)) + g(x)(f(x+Δx)-f(x)) ) $$\n",
    "  $$ = \\lim_{Δx\\to0} (\\frac{ΔA(x)}{Δx}) = \\lim_{Δx\\to0} ( \\frac{f(x)(g(x+Δx)-g(x)) + g(x)(f(x+Δx)-f(x))}{Δx} ) $$\n",
    "  $$ = \\lim_{Δx\\to0} (\\frac{ΔA(x)}{Δx}) = \\lim_{Δx\\to0} ( \\frac{f(x)(g(x+Δx)-g(x))}{Δx}+ \\frac{g(x)(f(x+Δx)-f(x))}{Δx} ) $$\n",
    "  $$ = \\lim_{Δx\\to0} (\\frac{ΔA(x)}{Δx}) = \\lim_{Δx\\to0} ( f(x)\\frac{(g(x+Δx)-g(x))}{Δx}+ g(x)\\frac{(f(x+Δx)-f(x))}{Δx} ) $$\n",
    "  $$ = \\lim_{Δx\\to0} (\\frac{ΔA(x)}{Δx}) = \\lim_{Δx\\to0} ( f(x)g'(x)+ g(x)f'(x) ) $$\n",
    "  $$ = A'(x) = f(x)g'(x)+ g(x)f'(x) $$\n",
    "* Based on this intuition, we can derive the formal product rule. It states that the derivative of a product of two functions, $f(x)g(x)$, is the sum of two terms: the first function times the derivative of the second, plus the second function times the derivative of the first.\n",
    "  $$ \\text{{Product Rule =  }}$$\n",
    "  $$ \\text{if } A(x) = f(x)g(x)$$\n",
    "  $$ \\text{then } A'(x) = f(x)g'(x)+g(x)f'(x)$$\n",
    "* This rule is a powerful tool in calculus and can be added to our toolbox alongside the Sum Rule and the Power Rule. It simplifies the process of differentiating complex functions that are the product of simpler ones.\n",
    "* Example: \n",
    "  * To differentiate $A(x)=xe^x\\cos(x)$, we apply the product rule for three functions from the previous question.\n",
    "  * Let $f(x) = x$,$g(x)=e^x$, and $h(x)=\\cos(x)$.\n",
    "    * $f'(x) = 1$\n",
    "    * $g'(x) = e^x$\n",
    "    * $h'(x) = -\\sin(x)$\n",
    "  * Applying the three-function product rule $A'(x) = f'(x)g(x)h(x) + f(x)g'(x)h(x) + f(x)g(x)h'(x)$:\n",
    "    $$ f'(x)g(x)h(x) = 1e^x\\cos(x) = e^x\\cos(x)$$\n",
    "    $$ f(x)g'(x)h(x) = xe^x\\cos(x) $$\n",
    "    $$ f(x)g(x)h'(x) = xe^x-\\sin(x)$$\n",
    "    $$ A'(x) = e^x\\cos(x) + xe^x\\cos(x) + xe^x-\\sin(x)$$\n",
    "    $$ A'(x) = e^x[\\cos(x) + x\\cos(x) - x\\sin(x)]$$\n",
    "    $$ A'(x) = e^x[(1+x)\\cos(x) - x\\sin(x)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c009bca",
   "metadata": {},
   "source": [
    "## A4. The Chain Rule of Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db15338",
   "metadata": {},
   "source": [
    "* The Chain Rule is the essential tool for differentiating composite functions, where one function is nested inside another (e.g., $h(p(m))$). This rule is the fourth and final tool needed to tackle complex differentiation problems.\n",
    "  1. Conceptualizing the chain\n",
    "    * A composite function relates an ultimate output to a final input through a chain of intermediate variables. This structure is common in science and engineering.\n",
    "    * Example: Relating Happiness ($h$) to Money ($m$) via Pizza ($p$).\n",
    "      * $h$ is a function of $p$.\n",
    "      * $p$ is a function of $m$.\n",
    "      * Goal: Find the rate of change of happiness with respect to money, $\\frac{dh}{dm}$.\n",
    "    * Example functions:\n",
    "      * $h(p)=-\\frac{1}{3}p^2+p+\\frac{1}{5}$\n",
    "      * $p(m)=e^m-1$\n",
    "    * ![Chain rule](images/chain_rule.png)\n",
    "  2. The Chain Rule formula\n",
    "   * The Chain Rule provides an elegant approach by multiplying the derivatives of the successive functions. \n",
    "   * The derivative of the composite function is the product of the derivative of the outer function with respect to the intermediate variable, and the derivative of the intermediate variable with respect to the innermost variable. \n",
    "   * The formula is intuitively represented as a chain of derivative relationships:\n",
    "   $$ \\frac{dh}{dm}=\\frac{dh}{dp}\\frac{dp}{dm} $$\n",
    "  3. Application example\n",
    "   * Step 1: Differentiate the individual functions.\n",
    "     * $h(p)=-\\frac{1}{3}p^2+p+\\frac{1}{5} \\text{ → } \\frac{dh}{dp}=1-\\frac{2}{3}p$\n",
    "     * $p(m)=e^m-1 \\text{ → } \\frac{dp}{dm} =e^m$\n",
    "   * Step 2: Apply the Chain Rule and eliminate the intermediate variable.\n",
    "     * $\\frac{dh}{dm} = \\frac{dh}{dp}\\frac{dp}{dm} = (1-\\frac{2}{3}p) \\cdot e^m  $\n",
    "     * By substituting $p = e^m - 1$ back into the expression, we ensure the final derivative is only a function of $m$.\n",
    "     $$\\frac{dh}{dm} = \\left(1 - \\frac{2}{3}(e^m - 1)\\right) e^m$$\n",
    "     $$\\frac{dh}{dm} = \\frac{1}{3}e^m (5 - 2e^m)$$\n",
    "* The magic of the Chain Rule is that it works even when direct substitution is impossible, provided we know the derivatives of the individual functions within the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ce4ef",
   "metadata": {},
   "source": [
    "## A5. Combination of the 4 rules (Sum, Power, Product, and Chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306aede5",
   "metadata": {},
   "source": [
    "* Applying the Calculus toolbox\n",
    "  * The function to be differentiated is:\n",
    "  $$f(x) = \\frac{\\sin(2x^5 + 3x)}{e^{7x}}$$\n",
    "  * The core strategy is to decompose the scary function into manageable pieces and use the Product Rule as the final step.\n",
    "    1. Preparation: Rewriting as a product\n",
    "      * To avoid the Quotient Rule, we rewrite the fraction as a product using a negative exponent:\n",
    "        $$f(x) = \\underbrace{\\sin(2x^5 + 3x)}_{g(x)} \\cdot \\underbrace{e^{-7x}}_{h(x)}$$\n",
    "      * The derivative will be found by the Product Rule: $f'(x) = g'(x)h(x) + g(x)h'(x)$.\n",
    "    2. Part 1: Differentiating $g(x) = \\sin(2x^5 + 3x)$\n",
    "      * This is a classic Chain Rule scenario, $g(x) = g(u(x))$, where the inner function is $u(x)$.\n",
    "        ![Chain rule 2](images/chain_rule_2.png)\n",
    "      * Applying the Chain Rule: $\\frac{dg}{dx} = \\frac{dg}{du} \\cdot \\frac{du}{dx}$:\n",
    "      $$\\frac{dg}{dx} = \\cos(u) \\cdot (10x^4 + 3)$$\n",
    "      * Substituting $u = 2x^5 + 3x$ back:\n",
    "      $$g'(x) = \\cos(2x^5 + 3x) (10x^4 + 3)$$\n",
    "    3. Part 2: Differentiating $h(x) = e^{-7x}$\n",
    "      * This is also a Chain Rule scenario, $h(x) = h(v(x))$, where the inner function is $v(x)$.\n",
    "        ![Chain rule 3](images/chain_rule_3.png)\n",
    "      * Applying the Chain Rule: $\\frac{dh}{dx} = \\frac{dh}{dv} \\cdot \\frac{dv}{dx}$:\n",
    "      $$\\frac{dh}{dx} = e^v \\cdot (-7)$$\n",
    "      * Substituting $v = -7x$ back:\n",
    "      $$h'(x) = -7e^{-7x}$$\n",
    "    4. Final step: Applying the Product Rule\n",
    "      * Finally, apply the Product Rule: $f'(x) = g'(x)h(x) + g(x)h'(x)$.\n",
    "      $$f'(x) = \\left[\\cos(2x^5 + 3x)(10x^4 + 3)\\right] \\cdot \\left[e^{-7x}\\right] + \\left[\\sin(2x^5 + 3x)\\right] \\cdot \\left[-7e^{-7x}\\right]$$\n",
    "      * The expression can be slightly rearranged by factoring out $e^{-7x}$:\n",
    "      $$f'(x) = e^{-7x} \\left[ (10x^4 + 3)\\cos(2x^5 + 3x) - 7\\sin(2x^5 + 3x) \\right]$$\n",
    "  * This single example utilized all four differentiation rules: Power Rule, Sum Rule, Chain Rule, and Product Rule. As is common in coding, further algebraic simplification (optimization) is often deferred until necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1aec4",
   "metadata": {},
   "source": [
    "# B. Multivariate Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b377d9",
   "metadata": {},
   "source": [
    "## B1. Partial Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0bae4",
   "metadata": {},
   "source": [
    "* Variables, Constants, and Parameters\n",
    "  \n",
    "  In multivariate calculus, understanding the role of each term in a function is crucial for differentiation.\n",
    "  \n",
    "  * Dependent Variable ($y$): A variable whose value depends on the values of others (e.g., speed depends on time).\n",
    "  * Independent Variable ($x$): A variable that is controlled or chosen freely, and on which the dependent variable relies (e.g., time).\n",
    "  * Constants: Values that are fixed in the context of the problem (e.g., $\\pi$).\n",
    "  * Parameters: Variables that are considered constants during a standard differentiation but are often varied by an engineer or designer to explore a family of similar functions (e.g., car mass or drag coefficient in a specific context).\n",
    "  \n",
    "  Key Takeaway: What is a constant or a variable is context-dependent. In calculus, you can differentiate any term with respect to any other, provided the context makes sense.\n",
    "\n",
    "* Introduction to Partial Differentiation\n",
    "  \n",
    "  Partial differentiation is the method of applying the familiar rules of single-variable calculus to functions of multiple variables.\n",
    "\n",
    "  * The core rule is:\n",
    "\n",
    "    When differentiating a function with respect to a specific variable, treat all other variables as constants.\n",
    "\n",
    "  * The Partial Derivative Symbol\n",
    "\n",
    "    We use the curly symbol, $\\partial$ (read as \"partial\"), instead of the standard $d$, to signify that the function being differentiated has more than one variable.For a function $f(x, y, z)$, the partial derivative with respect to $x$ is written as $\\frac{\\partial f}{\\partial x}$.\n",
    "\n",
    "* Example: Mass of a Can ($m$)\n",
    "  \n",
    "  The mass of a metal can is a function of its design parameters: radius ($r$), height ($h$), wall thickness ($t$), and density ($\\rho$).\n",
    "\n",
    "  ![Mass Function](images/mass_function.png)\n",
    "\n",
    "  The mass function is:\n",
    "  $$m(r, h, t, \\rho) = (2\\pi r^2 t \\rho) + (2\\pi r h t \\rho)$$\n",
    "\n",
    "  * Partial Derivative with respect to Height ($h$)\n",
    "    \n",
    "    When calculating $\\frac{\\partial m}{\\partial h}$, we treat $r, t,$ and $\\rho$ as constants.\n",
    "    $$\\frac{\\partial m}{\\partial h} = \\frac{\\partial}{\\partial h} [2\\pi r^2 t \\rho] + \\frac{\\partial}{\\partial h} [2\\pi r h t \\rho]$$\n",
    "    * The first term ($2\\pi r^2 t \\rho$) does not contain $h$, so its derivative is 0.\n",
    "    * The second term ($2\\pi r t \\rho$ is the constant multiplier of $h$), so its derivative is $2\\pi r t \\rho$.\n",
    "    $$\\frac{\\partial m}{\\partial h} = 0 + 2\\pi r t \\rho$$\n",
    "    $$\\frac{\\partial m}{\\partial h} = 2\\pi r t \\rho$$\n",
    "\n",
    "    The result no longer contains $h$, as mass varies linearly with height (all else being equal).\n",
    "\n",
    "  * Partial Derivative with respect to Radius ($r$)\n",
    "  \n",
    "    When calculating $\\frac{\\partial m}{\\partial r}$, we treat $h, t,$ and $\\rho$ as constants.\n",
    "    $$\\frac{\\partial m}{\\partial r} = \\frac{\\partial}{\\partial r} [2\\pi r^2 t \\rho] + \\frac{\\partial}{\\partial r} [2\\pi r h t \\rho]$$\n",
    "    * For the first term, $\\frac{\\partial}{\\partial r} [2\\pi t \\rho \\cdot r^2] = 2\\pi t \\rho \\cdot (2r) = 4\\pi r t \\rho$.\n",
    "    * For the second term, $\\frac{\\partial}{\\partial r} [2\\pi h t \\rho \\cdot r] = 2\\pi h t \\rho$.\n",
    "    $$\\frac{\\partial m}{\\partial r} = 4\\pi r t \\rho + 2\\pi h t \\rho$$\n",
    "\n",
    "  * Partial Derivative with respect to Thickness ($t$)\n",
    "    $$\\frac{\\partial m}{\\partial t} = \\frac{\\partial}{\\partial t} [2\\pi r^2 \\rho \\cdot t] + \\frac{\\partial}{\\partial t} [2\\pi r h \\rho \\cdot t]$$\n",
    "    $$\\frac{\\partial m}{\\partial t} = 2\\pi r^2 \\rho + 2\\pi r h \\rho$$\n",
    "\n",
    "  * Partial Derivative with respect to Density ($\\rho$)\n",
    "    $$\\frac{\\partial m}{\\partial \\rho} = \\frac{\\partial}{\\partial \\rho} [2\\pi r^2 t \\cdot \\rho] + \\frac{\\partial}{\\partial \\rho} [2\\pi r h t \\cdot \\rho]$$\n",
    "    $$\\frac{\\partial m}{\\partial \\rho} = 2\\pi r^2 t + 2\\pi r h t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f607b3",
   "metadata": {},
   "source": [
    "## B2. Total Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a0090",
   "metadata": {},
   "source": [
    "* Partial differentiation is the process of finding the derivative of a multivariate function with respect to one variable while treating all others as constants.\n",
    "  * Consider the function: $f(x, y, z) = \\sin(x)e^{yz^2}$\n",
    "    * With respect to $x$ ($\\frac{\\partial f}{\\partial x}$)\n",
    "      * Treat $e^{yz^2}$ as a constant. Differentiate $\\sin(x)$ to $\\cos(x)$.\n",
    "      * $\\frac{\\partial f}{\\partial x} = \\cos(x) e^{yz^2}$\n",
    "    * With respect to $y$ ($\\frac{\\partial f}{\\partial y}$)\n",
    "      * Treat $\\sin(x)$ as a constant. Apply the Chain Rule to $e^{yz^2}$ by multiplying by the derivative of the exponent w.r.t $y$, which is $z^2$.\n",
    "      * $\\frac{\\partial f}{\\partial y} = \\sin(x) e^{yz^2} (z^2)$\n",
    "    * With respect to $z$ ($\\frac{\\partial f}{\\partial z}$)\n",
    "      * Treat $\\sin(x)$ as a constant. Apply the Chain Rule to $e^{yz^2}$ by multiplying by the derivative of the exponent w.r.t. $z$, which is $2yz$\n",
    "      * $\\frac{\\partial f}{\\partial z} = \\sin(x) e^{yz^2} (2yz)$\n",
    "* The Total Derivative\n",
    "  * The Total Derivative is used when the function $f$ depends on multiple variables ($x, y, z$), and those variables are themselves functions of a single other parameter, $t$ (e.g., $x(t), y(t), z(t)$). The goal is to find the overall rate of change of $f$ with respect to this single parameter, $\\frac{df}{dt}$.\n",
    "  * The total derivative is the sum of the changes contributed by each variable. It is a direct extension of the Chain Rule:\n",
    "  $$\\frac{df}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} + \\frac{\\partial f}{\\partial z} \\frac{dz}{dt}$$\n",
    "  * The power of this rule is that it works even when direct substitution of all $t$-expressions into $f$ is algebraically too complex or impossible (if an analytical expression for $f$ is unavailable).\n",
    "  * Application Example\n",
    "  $$ f(x,y,z) = \\sin{(x)}e^{yz^2}$$\n",
    "    * Given the dependencies: $x(t) = t-1$, $y(t) = t^2$, $z(t) = 1/t = t^{-1}$\n",
    "  $$ f(t) = \\sin{(t-1)}e^{t^2(\\frac{1}{t})^2}$$\n",
    "  $$ f(t) = \\sin{(t-1)}e$$\n",
    "  $$ \\frac{df}{dt} = \\cos{(t-1)}e$$\n",
    "    * Now we know that:\n",
    "      * $\\frac{\\partial f}{\\partial x} = \\cos(x) e^{yz^2}$, and $\\frac{dx}{dt} = 1$\n",
    "      * $\\frac{\\partial f}{\\partial y} = \\sin(x) e^{yz^2} (z^2)$, and $\\frac{dy}{dt} = 2t$\n",
    "      * $\\frac{\\partial f}{\\partial z} = \\sin(x) e^{yz^2} (2yz)$, and $\\frac{dz}{dt} = -t^{-2}$\n",
    "    $$\\frac{df(x,y,z)}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} + \\frac{\\partial f}{\\partial z} \\frac{dz}{dt}$$\n",
    "    $$\\frac{df(x,y,z)}{dt} = \\cos(x) e^{yz^2} \\cdot 1 + \\sin(x) e^{yz^2} (z^2) \\cdot 2t + \\sin(x) e^{yz^2} (2yz) \\cdot -t^{-2}$$\n",
    "    * We substitute $x$, $y$, $z$ all interms of $t$, $x(t) = t-1$, $y(t) = t^2$, $z(t) = 1/t = t^{-1}$\n",
    "    $$\\frac{df(x,y,z)}{dt} = \\cos(t-1)e + t^{-2}\\sin{(t-1)}e \\cdot 2t + 2t \\sin{(t-1)} e \\cdot (-t^{-2}) $$\n",
    "    $$\\frac{df(x,y,z)}{dt} = \\cos(t-1)e + 2t^{-1}\\sin{(t-1)}e  -2t^{-1} \\sin{(t-1)}e $$\n",
    "    $$\\frac{df(x,y,z)}{dt} = \\cos(t-1)e $$\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe636193",
   "metadata": {},
   "source": [
    "## B3. Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03641e9c",
   "metadata": {},
   "source": [
    "* The Jacobian is a vector that brings together the partial derivatives of a multivariate function into a single, useful structure. This concept is fundamental to optimization and machine learning, particularly when dealing with functions of many variables.\n",
    "1. Definition of the Jacobian Vector ($J$)\n",
    "   \n",
    "   For a single function $f$ that depends on multiple variables, $f(x_1, x_2, x_3, \\dots)$, the Jacobian is simply a row vector where each entry is the partial derivative of $f$ with respect to each variable in turn.\n",
    "   \n",
    "   The Jacobian $J$ is formally defined as:\n",
    "   $$J = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\frac{\\partial f}{\\partial x_3} & \\dots \\end{bmatrix}$$\n",
    "   \n",
    "2. Worked Example\n",
    "   \n",
    "   Consider the function: $f(x, y, z) = x^2y + 3z$.\n",
    "   \n",
    "   To construct the Jacobian, we find each partial derivative:\n",
    "   * $\\frac{\\partial f}{\\partial x}$ (Treat $y, z$ as constants): $2xy$\n",
    "   * $\\frac{\\partial f}{\\partial y}$ (Treat $x, z$ as constants): $x^2$\n",
    "   * $\\frac{\\partial f}{\\partial z}$ (Treat $x, y$ as constants): $3$\n",
    "  \n",
    "   Combining these results gives the Jacobian vector $J$:\n",
    "   $$J = \\begin{bmatrix} 2xy & x^2 & 3 \\end{bmatrix}$$\n",
    "\n",
    "3. Geometric Interpretation\n",
    "   \n",
    "   The Jacobian vector is crucial because it provides the local properties of the function at any given point $(x, y, z)$:\n",
    "   * Direction of Steepest Slope: When evaluated at a specific point, the Jacobian vector points in the direction of the steepest uphill slope (or gradient) of the function.\n",
    "   * Magnitude (Steepness): The magnitude (length) of the Jacobian vector at that point is equal to the steepness of the function at that location. The tighter the contour lines are packed, the larger the magnitude of the Jacobian.\n",
    "   * Flat Regions: At the peaks, valleys, or flat plains of a function, the gradient is zero, and the magnitude of the Jacobian vector will be small (or zero).\n",
    "* Understanding the Jacobian graphically (e.g., on a contour plot) allows us to visualize the gradient in 2D or 3D, providing the necessary intuition for tackling higher-dimensional problems later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac9c9e",
   "metadata": {},
   "source": [
    "4. Recap: The Jacobian Vector and its Meaning\n",
    "* The Jacobian of a single function $f(x_1, x_2, \\dots)$ is a vector composed of its partial derivatives. It provides critical information about the function's local behavior:\n",
    "  * Direction: The Jacobian vector, $\\mathbf{J}$, points in the direction of the steepest uphill slope (the gradient).\n",
    "  * Magnitude: The magnitude of $\\mathbf{J}$ represents the steepness of the function at that specific point.\n",
    "  * Example: $f(x, y) = e^{-(x^2 + y^2)}$\n",
    "    * The Jacobian points toward the origin $(0, 0)$.\n",
    "    * $\\mathbf{J} = [-2xe^{-(x^2+y^2)},-2ye^{-(x^2+y^2)}]$ \n",
    "    * $\\mathbf{J}(-1,1) = [2e^{-2},-2e^{-2}] = [0.27, -0.27]$ \n",
    "    * $\\mathbf{J}(2,2) = [-0.001, -0.001]$ \n",
    "    * At the origin, $\\mathbf{J}(0, 0) = \\mathbf{0}$, indicating the function is flat (a maximum, minimum, or saddle point). Visualizing the function confirms the origin is a maximum.\n",
    "    * ![Jacobian](images/jacobian_1.png)\n",
    "5. Extending to the Jacobian Matrix\n",
    "* The Jacobian Matrix allows us to describe the rates of change for a vector-valued function—a function that takes a vector as an input and returns a vector as an output.\n",
    "  \n",
    "  If we have two output functions, $u$ and $v$, that are both functions of $x$ and $y$:\n",
    "    $$\\mathbf{F}(x, y) = \\begin{bmatrix} u(x, y) \\\\ v(x, y) \\end{bmatrix}$$\n",
    "  \n",
    "  The Jacobian Matrix, $\\mathbf{J}$, is formed by stacking the Jacobian vectors of the individual output functions ($u$ and $v$) as rows:\n",
    "    $$\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{bmatrix}$$\n",
    "* Example: Linear Vector Field\n",
    "* Consider the linear vector-valued function with components:\n",
    "  * $u(x, y) = x - 2y$\n",
    "  * $v(x, y) = 3y - 2x$\n",
    "* The Jacobian matrix $\\mathbf{J}$ is:\n",
    "  $$\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 1 & -2 \\\\ -2 & 3 \\end{bmatrix}$$\n",
    "* Since $u$ and $v$ are linear functions, the partial derivatives (gradients) are constant everywhere. This Jacobian matrix represents the linear transformation from the $(x, y)$ space to the $(u, v)$ space.\n",
    "* Application: Non-Linear Transformations\n",
    "* For highly nonlinear functions, the Jacobian matrix is a crucial tool because:\n",
    "  * Linear Approximation: If the function is smooth, we can zoom in close enough to consider a small region of space to be approximately linear. The Jacobian matrix provides the best linear approximation of the function's behavior at that point.\n",
    "  * Transformation Scaling: The determinant of the Jacobian matrix, $|\\mathbf{J}|$, quantifies how a small region of space changes in size (scales) when it is transformed by the function.\n",
    "* A classic example is transforming between coordinate systems, such as Cartesian $(x, y)$ and Polar $(r, \\theta)$ coordinates, where the Jacobian determinant $\\left|\\frac{\\partial(x, y)}{\\partial(r, \\theta)}\\right| = r$ is used to correctly calculate areas and volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e88358d",
   "metadata": {},
   "source": [
    "## B4. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28270b9",
   "metadata": {},
   "source": [
    "* Optimization in mathematics refers to the process of finding the input values that correspond to the maximum or minimum output values of a function. This is vital in fields like city planning, scheduling, and machine learning.\n",
    "* The Jacobian serves as our primary tool in optimization, acting as a \"torch\" in the dark.\n",
    "1. Optimization Targets\n",
    "* Functions often have multiple locations with zero gradient ($J = \\mathbf{0}$). These critical points are categorized based on their functional value across the entire domain:\n",
    "  * ![Optimization](images/optimization.png)\n",
    "  * Global Maximum: The single highest peak of the function (e.g., Point A).\n",
    "  * Local Maxima: Peaks that are higher than their immediate surrounding area, but not the highest overall (e.g., Points C and E).\n",
    "  * Global Minimum: The single deepest trough of the function (e.g., Point D).\n",
    "  * Local Minima: Troughs that are lower than their immediate surrounding area, but not the lowest overall (e.g., Point B).\n",
    "2. The Jacobian as a Guide (The \"Night Walk\" Analogy)\n",
    "* The Jacobian vector is essential when we do not have an analytical expression for the function (the \"night-time scenario,\" common when outputs come from complex simulations or experiments).\n",
    "  * Guidance: The Jacobian vector always points uphill (in the direction of the steepest ascent).\n",
    "  * Limitation: While the Jacobian points uphill, it does not guarantee the path leads to the Global Maximum. Following the local gradient often leads only to the nearest Local Maximum.\n",
    "  * At Critical Points: At any maximum or minimum (local or global), the gradient is zero, meaning the Jacobian vector is the zero vector ($\\mathbf{J} = \\mathbf{0}$).\n",
    "3. The Sandpit Analogy\n",
    "* While the \"night-time hill walk\" helps visualize the local direction, a better analogy for the process is the sandpit with an uneven base exercise:\n",
    "  * The Task: Find the deepest point (Global Minimum) of the pit by only measuring the depth at various points using a long stick.\n",
    "  * The Constraint: You cannot see the overall landscape (no analytical expression/plot). You must rely on measurements at isolated points.\n",
    "  * The Reality: Unlike walking, calculating $f(x)$ at $x_1$ and then jumping to $x_2$ has the same cost, regardless of the distance between them (no \"walking\" time is incurred).\n",
    "* This analogy highlights the central challenge in optimization: how to efficiently explore a high-dimensional space to find the global optimum without getting stuck at a local optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef151d",
   "metadata": {},
   "source": [
    "## B5. Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b853055",
   "metadata": {},
   "source": [
    "The Hessian is the matrix of all second-order partial derivatives of a multivariate function. It is a fundamental tool used in optimization to determine the nature of critical points (maxima, minima, or saddle points).\n",
    "1. Definition of the Hessian Matrix\n",
    "  * The Hessian Matrix, $\\mathbf{H}$, is formed by collecting all possible second-order partial derivatives of a scalar-valued function $f$ with $n$ variables ($x_1, x_2, \\dots, x_n$).\n",
    "  * For a function $f(x_1, x_2, \\dots, x_n)$, the Hessian is an $n \\times n$ square matrix where the entry in the $i$-th row and $j$-th column is:\n",
    "  $$H_{ij} = \\frac{\\partial}{\\partial x_j} \\left(\\frac{\\partial f}{\\partial x_i}\\right) = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i}$$\n",
    "  * The Hessian Structure (for $n=3$ variables $x, y, z$):\n",
    "  $$\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial z \\partial x} \\\\ \\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial y^2} & \\frac{\\partial^2 f}{\\partial z \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial x \\partial z} & \\frac{\\partial^2 f}{\\partial y \\partial z} & \\frac{\\partial^2 f}{\\partial z^2} \\end{bmatrix}$$\n",
    "  \n",
    "  * Key Property: Symmetry\n",
    "  * If the function $f$ is continuous (has no sudden step changes), the Hessian matrix will always be symmetrical across the leading diagonal. This means the mixed partial derivatives are equal:$$\\frac{\\partial^2 f}{\\partial y \\partial x} = \\frac{\\partial^2 f}{\\partial x \\partial y}$$\n",
    "2. Worked Example: Building the Hessian\n",
    "  * Consider the function $f(x, y, z) = x^2yz$.\n",
    "  * Step 1: Find the Jacobian (First-Order Derivatives)\n",
    "  $$\\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} & \\frac{\\partial f}{\\partial z} \\end{bmatrix} = \\begin{bmatrix} 2xyz & x^2z & x^2y \\end{bmatrix}$$\n",
    "  * Step 2: Differentiate the Jacobian terms againThe elements of the Jacobian become the rows of the Hessian:$$\\mathbf{H} = \\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial x}(2xyz) & \\frac{\\partial}{\\partial y}(2xyz) & \\frac{\\partial}{\\partial z}(2xyz) \\\\\n",
    "\\frac{\\partial}{\\partial x}(x^2z) & \\frac{\\partial}{\\partial y}(x^2z) & \\frac{\\partial}{\\partial z}(x^2z) \\\\\n",
    "\\frac{\\partial}{\\partial x}(x^2y) & \\frac{\\partial}{\\partial y}(x^2y) & \\frac{\\partial}{\\partial z}(x^2y)\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "2yz & 2xz & 2xy \\\\\n",
    "2xz & 0 & x^2 \\\\\n",
    "2xy & x^2 & 0\n",
    "\\end{bmatrix}$$(Note the symmetry: $H_{12}=H_{21}$, $H_{13}=H_{31}$, $H_{23}=H_{32}$)\n",
    "\n",
    "3. Using the Hessian for OptimizationThe Hessian is essential for classifying critical points where the Jacobian (gradient) is zero ($\\mathbf{J}=\\mathbf{0}$).For a critical point, we use the Hessian to determine if it is a maximum, a minimum, or a saddle point. For simplicity, we can look at a 2D function, $f(x, y)$:$$\\mathbf{H} = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial y \\partial x} \\\\ \\frac{\\partial^2 f}{\\partial x \\partial y} & \\frac{\\partial^2 f}{\\partial y^2} \\end{bmatrix}$$\n",
    "\n",
    "![Hessian](images/hessian.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba16a89",
   "metadata": {},
   "source": [
    "## B6. Real-World Challenges in Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59725a64",
   "metadata": {},
   "source": [
    "1. Real-World Challenges in Optimization\n",
    "* The theoretical tools (Jacobian, Hessian) are powerful, but their application to real systems introduces several challenges:\n",
    "  * High Dimensionality: Many problems, like training neural networks, involve hundreds or thousands of dimensions. We must trust the math and rely on our 2D/3D intuition to guide us, as we can no longer visualize the function as a landscape.\n",
    "  * Unknown or Expensive Functions:\n",
    "    * The function $f(x)$ may not have a nice analytical expression (a simple formula).\n",
    "    * Calculating a single function value (a single \"depth measurement\" in the sandpit) might be computationally expensive (requiring supercomputer time or extensive lab work), making it impossible to fully plot the surface.\n",
    "  * Non-Smoothness: Real-world functions may contain sharp features or discontinuities, making traditional differentiation rules invalid at those points.\n",
    "  * Noisy Data: If the function measurements are noisy, the calculated Jacobian vectors can become highly unreliable unless carefully handled (e.g., averaging).\n",
    "2. The Challenge of the Unknown Jacobian\n",
    "* The central question for real-world optimization is: If we don't have an analytical expression for the function $f(x)$, how can we calculate the Jacobian (partial derivatives)?\n",
    "* The answer lies in Numerical Methods, which provide approximate solutions when exact ones are intractable or unknown.\n",
    "* The Finite Difference Method\n",
    "  * The finite difference method takes us back to the original definition of the derivative (rise over run) over a finite interval. Since we cannot calculate the limit as the interval approaches zero, we use an approximation based on available function values.\n",
    "  * To approximate the partial derivative $\\frac{\\partial f}{\\partial x_i}$ at a starting location $\\mathbf{x}$, we:\n",
    "    * Take a small step size, $\\Delta x_i$.\n",
    "    * Calculate the function value at the starting point, $f(\\mathbf{x})$.\n",
    "    * Calculate the function value after taking a small step in the $x_i$ direction, $f(\\mathbf{x} + \\Delta x_i)$.\n",
    "  * The partial derivative is approximated by the slope over this finite interval:\n",
    "  $$\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(\\mathbf{x} + \\Delta x_i) - f(\\mathbf{x})}{\\Delta x_i}$$\n",
    "  * The Jacobian is then constructed by approximating each partial derivative in turn (taking a small step in $x$, then a small step in $y$, etc.).\n",
    "* Practical Considerations for Step Size ($\\Delta x_i$)\n",
    "  * Choosing the right step size is a balance:\n",
    "    * Too Big: A large step size leads to a poor approximation of the instantaneous slope (high approximation error).\n",
    "    * Too Small: A too-small step size can lead to numerical issues. Computers store values with limited significant figures; if the change in the function value ($f(\\mathbf{x} + \\Delta x_i) - f(\\mathbf{x})$) is smaller than the computer's precision, the calculated change might be zero, yielding an inaccurate derivative.\n",
    "* Dealing with Noise\n",
    "  * If the data is noisy, a simple approach to approximate the gradient robustly is to calculate the finite difference using several different step sizes and then average the resulting gradient approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae9015",
   "metadata": {},
   "source": [
    "# C. Multivariate Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d9aae6",
   "metadata": {},
   "source": [
    "## C1. Multivariate Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f63a5",
   "metadata": {},
   "source": [
    "* We previously established the **Total Derivative** for a function of three variables $f(x, y, z)$ where each variable depends on $t$. \n",
    "$$f(x,y,z)=\\sin(x)e^{yz^2}$$\n",
    "$$x=t-1, y=t^2, z=\\frac{1}{t}$$\n",
    "$$\\frac{df}{dt}=\\frac{\\partial f}{\\partial x}\\frac{dx}{dt}+\\frac{\\partial f}{\\partial y}\\frac{dy}{dt}+\\frac{\\partial f}{\\partial z}\\frac{dz}{dt}$$\n",
    "$$\\frac{df(x,y,z)}{dt}=\\cos(t-1)e$$\n",
    "* We can use this expression which is simply the sum of the chains relating $f$ to $t$ through each of its three variables. This allowed us to calculate the result in a Piecewise manner rather than substituting everything in at the start.\n",
    "* By generalizing this concept to $n$ variables using linear algebra, we can simplify the notation and computation significantly.\n",
    "1. Vector Notation for Multivariate Functions\n",
    "    * Instead of writing a function with a long list of inputs like $f(x_1, x_2, ..., x_n) = f(\\mathbf{x})$, we collect these variables into a single vector, denoted by a bold $\\mathbf{x}$.\n",
    "    * The Input Vector: $\\mathbf{x} = [x_1, x_2, ..., x_n]$\n",
    "    * The Function: $f(\\mathbf{x})$\n",
    "    * The Dependency: Each component of the vector $\\mathbf{x}$ is itself a function of an independent variable $t$.\n",
    "2. Generalizing the Chain Rule\n",
    "    * To find the total derivative $\\frac{df}{dt}$, we need to sum the chains linking $f$ to $t$ through each of the $n$ variables. We can organize the necessary derivatives into two vectors:\n",
    "      * The Partial Derivatives of $f$: A vector containing how $f$ changes with respect to each component of $\\mathbf{x}$ (often called the gradient).\n",
    "        $$\\left[ \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right]$$\n",
    "      * The Derivatives of $\\mathbf{x}$: A vector containing how each component of $\\mathbf{x}$ changes with respect to $t$.\n",
    "        $$\\left[ \\frac{dx_1}{dt}, \\frac{dx_2}{dt}, \\dots, \\frac{dx_n}{dt} \\right]$$\n",
    "3. The Dot Product Formulation\n",
    "    * The multivariate chain rule requires the sum of the products of the corresponding terms in these two vectors. In linear algebra, this operation is the Dot Product.\n",
    "    * Therefore, the cumbersome summation formula simplifies to the dot product of the partial derivative vector and the time derivative vector.\n",
    "    $$\\frac{df}{dt} = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2}  \\\\ \\frac{\\partial f}{\\partial x_3}\\\\... \\\\ \\frac{\\partial f}{\\partial x_n}\\end{bmatrix} \\cdot \\begin{bmatrix} \\frac{dx_1}{d_t} \\\\ \\frac{dx_2}{d_t} \\\\ \\frac{dx_3}{d_t} \\\\...\\\\\\frac{dx_n}{d_t}\\end{bmatrix} = \\frac{\\partial f}{\\partial x} \\cdot \\frac{dx}{dt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e83b7a",
   "metadata": {},
   "source": [
    "## C2. Multivariate Chain Rule: Jacobians and Extended Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813b95f",
   "metadata": {},
   "source": [
    "1. The Jacobian Perspective\n",
    "   * In the previous module, we established that for a scalar function $f(\\mathbf{x})$ where the vector $\\mathbf{x}$ depends on $t$, the derivative is the dot product of the gradient and the velocity vector.\n",
    "   * We can express this more elegantly using linear algebra and the Jacobian:\n",
    "     * The Gradient ($\\frac{\\partial f}{\\partial \\mathbf{x}}$): Often written as a column vector.\n",
    "     * The Jacobian ($J_f$): For a scalar function, the Jacobian is a row vector (the transpose of the gradient).\n",
    "    $$J_f = \\left[ \\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right]$$\n",
    "     * The Chain Rule: Since the dot product of two column vectors is mathematically equivalent to multiplying a row vector by a column vector, we can write the chain rule as:\n",
    "    $$\\frac{df}{dt} = J_f \\cdot \\frac{d\\mathbf{x}}{dt}$$\n",
    "   * This notation confirms that the Jacobian is the most convenient representation for handling multivariate derivatives.\n",
    "2. Extending the Chain (Univariate Example)\n",
    "    * The chain rule applies regardless of how many links are in the chain. Consider a scenario with three layers of dependency: $f \\rightarrow x \\rightarrow u \\rightarrow t$.\n",
    "    * Example:\n",
    "      * $f(x) = 5x$\n",
    "      * $x(u) = 1 - u$\n",
    "      * $u(t) = t^2$\n",
    "    * We can solve this in two ways:\n",
    "      * Substitution: Combine functions to get $f(t) = 5(1 - t^2) = 5 - 5t^2$, then differentiate to get $-10t$.\n",
    "      * Chain Rule: Multiply the derivatives of each link.\n",
    "        $$\\frac{df}{dt} = \\frac{df}{dx} \\cdot \\frac{dx}{du} \\cdot \\frac{du}{dt}$$\n",
    "        $$= (5) \\cdot (-1) \\cdot (2t) = -10t$$\n",
    "    * Both methods yield the same result, proving the chain rule holds for longer sequences.\n",
    "3. The Multivariate Extension ($f(\\mathbf{x}(\\mathbf{u}(t)))$)\n",
    "    * Now, consider the complex case where we calculate the derivative of a scalar $f$ with respect to $t$, but via two intermediate vector-valued functions:\n",
    "      $$t \\rightarrow \\mathbf{u} \\rightarrow \\mathbf{x} \\rightarrow f$$\n",
    "    * To find $\\frac{df}{dt}$, we multiply the derivatives of each stage. However, we must pay attention to the dimensions of the objects involved:\n",
    "      * Stage 1 ($f \\leftarrow \\mathbf{x}$): $f$ is a scalar, $\\mathbf{x}$ is a vector. The derivative is the Jacobian Row Vector ($J_f$).\n",
    "        $$J_f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} & \\dots & \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} \\quad (1 \\times n)$$\n",
    "      * Stage 2 ($\\mathbf{x} \\leftarrow \\mathbf{u}$): $\\mathbf{x}$ is a vector, $\\mathbf{u}$ is a vector. The derivative of a vector with respect to another vector forms a Jacobian Matrix ($J_\\mathbf{x}$).\n",
    "        $$J_\\mathbf{x} = \\begin{bmatrix} \\frac{\\partial x_1}{\\partial u_1} & \\dots \\\\ \\vdots & \\ddots \\end{bmatrix} \\quad (n \\times m)$$\n",
    "      * Stage 3 ($\\mathbf{u} \\leftarrow t$): $\\mathbf{u}$ is a vector, $t$ is a scalar. The derivative is a Column Vector ($\\frac{d\\mathbf{u}}{dt}$).\n",
    "        $$\\frac{d\\mathbf{u}}{dt} = \\begin{bmatrix} du_1/dt \\\\ \\vdots \\\\ du_m/dt \\end{bmatrix} \\quad (m \\times 1)$$\n",
    "      * (Dimensionally) If we multiply the derivatives of each stage:\n",
    "        $$(1 \\times 1) = (1 \\times n) \\cdot (n \\times m) \\cdot (m \\times 1)$$\n",
    "    * The Formula\n",
    "      $$\\frac{df}{dt} = J_f \\cdot J_\\mathbf{x} \\cdot \\frac{d\\mathbf{u}}{dt}$$\n",
    "    * Dimensional Consistency\n",
    "      * The power of Linear Algebra ensures these components fit together perfectly to return a single scalar value:\n",
    "        $$\\underbrace{\\frac{df}{dt}}_{\\text{Scalar } (1 \\times 1)} = \\underbrace{J_f}_{\\text{Row } (1 \\times n)} \\cdot \\underbrace{J_\\mathbf{x}}_{\\text{Matrix } (n \\times m)} \\cdot \\underbrace{\\frac{d\\mathbf{u}}{dt}}_{\\text{Column } (m \\times 1)}$$\n",
    "      * This structure allows us to handle complex, multi-layered systems (like deep neural networks) by breaking them down into manageable matrix operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
