# Mathematics for Machine Learning – Notes & Python Visualizations 📐

This repository contains my personal learning notes, Python implementations, and visual explanations based on the **Mathematics for Machine Learning Specialization** by Imperial College London.

> The goal of this project is to build strong mathematical intuition for machine learning, reinforced through code and visual storytelling.

---

## 🔍 Specialization Overview

This 3-course series bridges the gap between academic math and its applications in Data Science & Machine Learning. Each course combines theory with hands-on Jupyter notebooks and interactive visualizations.

| Course | Title                              | Description                                                                                |
| ------ | ---------------------------------- | ------------------------------------------------------------------------------------------ |
| 📘 1    | [Linear Algebra](https://github.com/JohnsonIsHere/mathematics-for-machine-learning-notes/tree/main/linear-algebra)                     | Understand vectors, matrices, basis transformations, and how they relate to ML             |
| 📗 2    | Multivariate Calculus              | Learn optimization, gradients, chain rule, and backpropagation                             |
| 📙 3    | Principal Component Analysis (PCA) | Use linear algebra & calculus to perform dimensionality reduction on high-dimensional data |

🧑‍🏫 **Instructors:**  
David Dye ・ Samuel J. Cooper ・ Marc Peter Deisenroth ・ A. Freddie Page  
*Imperial College London*

---

## 📚 Chapters & Learning Notes

### 📘 Mathematics for Machine Learning: Linear Algebra

- A. Introduction to Linear Algebra  
  - A1. What is a vector?  
  - A2. Vector addition  
  - A3. Vector multiplication  

- B. Vectors and Dot Products  
  - B1. Length of vectors  
  - B2. Dot product  
  - B3. Cosine rule  
  - B4. Vector projection  
  - B5. Changing basis  
  - B6. Vector space and linear independence  

- C. Matrices  
  - C1. What is a matrix?  
  - C2. Matrix operations  
  - C3. Composition of transformations  
  - C4. Matrix inverses  
  - C5. Determinants & special matrices  

> 🧠 Upcoming:
> - D. Matrices make linear mappings
> - E. Eigenvalues and Eigenvectors

---

### 📗 Mathematics for Machine Learning: Multivariate Calculus

- A. Introduction to Functions  
- B. Limits and Continuity  
- C. Partial Derivatives  
- D. Gradient Vectors  
- E. Chain Rule in Multiple Dimensions  
- F. Jacobian and Hessian  
- G. Optimization & Critical Points  
- H. Backpropagation & Application in Neural Networks  

> 🛠️ Coming soon

---

### 📙 Mathematics for Machine Learning: PCA (Dimensionality Reduction)

- A. What is PCA and why it matters  
- B. Covariance matrices & eigen decomposition  
- C. Eigenvalues and eigenvectors  
- D. Explained variance  
- E. Dimensionality reduction in practice  
- F. PCA on real-world data (e.g., MNIST)  
- G. Python implementation using NumPy and Scikit-learn  

> 🛠️ Coming soon
---

## 💻 Tools Used

- Python 3.x
- Jupyter Notebook
- NumPy, Matplotlib
- SymPy (for symbolic math)
- Scikit-learn (for PCA)

---

## 🧠 Author

**JT Y.** – Data Scientist & M.Sc. student in Project Management & Data Science @ HTW Berlin  
[LinkedIn](https://www.linkedin.com/in/jt-y-37a299174/) ・ [Medium](https://medium.com/@johnsonxxx0926) ・ [GitHub](https://github.com/JohnsonIsHere)

> "Mathematics is the bridge between the abstract and the applicable — and machine learning walks right on it."

---

## ⭐ Related Projects

- [Essential Math for Data Science 📘](https://github.com/JohnsonIsHere/essential-math-ds-notes)

---

## 📜 License

MIT License © 2025 JT Y.
